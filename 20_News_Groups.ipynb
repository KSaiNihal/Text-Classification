{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSaiNihal/Text-Classification-20_News_Groups-/blob/main/20_News_Groups.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, stem_text\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "jcSqUWxlaXL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to the dataset\n",
        "url = \"http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\"\n",
        "\n",
        "# Download the dataset\n",
        "response = requests.get(url)\n",
        "with open(\"20news-19997.tar.gz\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(\"20news-19997.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall()"
      ],
      "metadata": {
        "id": "RHKRBOwAaXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "target = []\n",
        "\n",
        "# Adjust the directory path according to where your dataset is located\n",
        "dataset_dir = \"/content/20news-bydate-train\"  # Update this path\n",
        "\n",
        "for category in os.listdir(dataset_dir):\n",
        "    category_path = os.path.join(dataset_dir, category)\n",
        "    if os.path.isdir(category_path):\n",
        "        for document in os.listdir(category_path):\n",
        "            document_path = os.path.join(category_path, document)\n",
        "            with open(document_path, \"r\", errors=\"ignore\") as f:\n",
        "                data.append(f.read())\n",
        "            target.append(category)\n",
        "\n",
        "# Create a DataFrame using pandas\n",
        "df = pd.DataFrame({'text': data, 'target': target})\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed-KOmhSaXH-",
        "outputId": "eddb3332-4f92-466a-a24d-c8234965da81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text              target\n",
            "0   egsner!ernest!m2.dseg.ti.com!tilde.csc.ti.com...  rec.sport.baseball\n",
            "1  From: rachford@en.ecn.purdue.edu (Jeffery M Ra...  rec.sport.baseball\n",
            "2  From: jtchern@ocf.berkeley.edu (Joseph Hernand...  rec.sport.baseball\n",
            "3  From: gspira@nyx.cs.du.edu (Greg Spira)\\nSubje...  rec.sport.baseball\n",
            "4  From: klopfens@andy.bgsu.edu (Bruce Klopfenste...  rec.sport.baseball\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[1:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "collapsed": true,
        "id": "jvthUodGbV3M",
        "outputId": "ebb93094-121b-4879-a612-15f72603b139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     target\n",
              "1  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...  sci.space"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-543ab6ec-58b8-4fe6-8c00-b1f0c71fbb4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-543ab6ec-58b8-4fe6-8c00-b1f0c71fbb4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-543ab6ec-58b8-4fe6-8c00-b1f0c71fbb4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-543ab6ec-58b8-4fe6-8c00-b1f0c71fbb4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[1:2]\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"From: henry@zoo.toronto.edu (Henry Spencer)\\nSubject: Re: Big amateur rockets\\nOrganization: U of Toronto Zoology\\nLines: 30\\n\\nIn article <C5Ky9y.MKK@raistlin.udev.cdc.com> pbd@runyon.cim.cdc.com (Paul Dokas) writes:\\n>Anyhow, the ad stated that they'd sell rockets that were up to 20' in length\\n>and engines of sizes \\\"F\\\" to \\\"M\\\".  They also said that some rockets will\\n>reach 50,000 feet.\\n>\\n>Now, aside from the obvious dangers to any amateur rocketeer using one\\n>of these beasts, isn't this illegal?  I can't imagine the FAA allowing\\n>people to shoot rockets up through the flight levels of passenger planes.\\n\\nThe situation in this regard has changed considerably in recent years.\\nSee the discussion of \\\"high-power rocketry\\\" in the rec.models.rockets\\nfrequently-asked-questions list.\\n\\nThis is not hardware you can walk in off the street and buy; you need\\nproper certification.  That can be had, mostly through Tripoli (the high-\\npower analog of the NAR), although the NAR is cautiously moving to extend\\nthe upper boundaries of what it considers proper too.\\n\\nYou need special FAA authorization, but provided you aren't doing it under\\none of the LAX runway approaches or something stupid like that, it's not\\nespecially hard to arrange.\\n\\nAs with model rocketry, this sort of hardware is reasonably safe if handled\\nproperly.  Proper handling takes more care, and you need a lot more empty\\nair to fly in, but it's basically just model rocketry scaled up.  As with\\nmodel rocketry, the high-power people use factory-built engines, which\\neliminates the major safety hazard of do-it-yourself rocketry.\\n-- \\nAll work is one man's work.             | Henry Spencer @ U of Toronto Zoology\\n                    - Kipling           |  henry@zoo.toronto.edu  utzoo!henry\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"sci.space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking missing values"
      ],
      "metadata": {
        "id": "LjCXpc-8lj5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the count of missing values\n",
        "print(f\"Missing values:\\n{missing_values}\")\n",
        "\n",
        "# Optionally, handle missing values if present\n",
        "if missing_values.any():\n",
        "    # Example: Fill missing values with an empty string\n",
        "    df.fillna(\"\", inplace=True)\n",
        "    print(\"Missing values handled.\")\n",
        "\n",
        "# Confirm if there are any remaining missing values\n",
        "print(f\"After handling missing values:\\n{df.isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmVsUDr4b92r",
        "outputId": "ae04ddd2-fd9b-4160-da19-8d80825c392d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            "text      0\n",
            "target    0\n",
            "dtype: int64\n",
            "After handling missing values:\n",
            "text      0\n",
            "target    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Dulicates"
      ],
      "metadata": {
        "id": "AAOimvmulqAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates based on the 'text' column\n",
        "duplicates = df.duplicated(subset=['text'])\n",
        "\n",
        "# Count the number of duplicates\n",
        "num_duplicates = duplicates.sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "\n",
        "# Optionally, display the duplicate rows\n",
        "duplicate_rows = df[duplicates]\n",
        "print(\"Duplicate rows:\")\n",
        "print(duplicate_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_T6WIMcJ2D",
        "outputId": "ed3e0bd0-12e3-47a6-c517-2c4cc6a03731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n",
            "Duplicate rows:\n",
            "Empty DataFrame\n",
            "Columns: [text, target]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower casing and Removing numbers, puntuation, tags etc"
      ],
      "metadata": {
        "id": "oq4qLHTTmqA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "# df = pd.read_csv('20_newsgroups_combined.csv')\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    # Gensim preprocessing\n",
        "    filters = [\n",
        "        lambda x: x.lower(),           # Convert to lowercase\n",
        "        strip_tags,                    # Remove HTML tags\n",
        "        strip_numeric,                 # Remove numbers\n",
        "        strip_punctuation,             # Remove punctuation\n",
        "        strip_multiple_whitespaces     # Remove extra whitespaces\n",
        "    ]\n",
        "    text = ' '.join(preprocess_string(text, filters=filters))\n",
        "\n",
        "    # NLTK preprocessing\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply preprocessing steps to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Save the preprocessed dataset\n",
        "# df.to_csv('preprocessed_20_newsgroups_gensim.csv', index=False)\n",
        "\n",
        "# Print the result\n",
        "print(df['text'].iloc[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-xm_wgolFFx",
        "outputId": "f30c49e7-0ff9-42f1-81ec-0ae096b9a1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nsmca aurora alaska edu subject eco freaks forcing space mining article aurora apr organization university alaska fairbanks lines nntp posting host acad alaska edu article prb access digex com pat writes article nsmca aurora alaska edu writes article prb access digex com pat writes besides line horse puckey mining companies claimed told pay restoring land strip mining aint talking large even mining companies talking small miners people employees people go every year set thier sluice box mining semi old fashion way okay use modern methods toa point lot small miners longer miners people living rent free federal land claim miner facts many people sustaint heir income mining often even live full time fotentimes fair bit environmental damage minign statutes created inthe west uninhabited designed bring people frontier times change people change deal constitutional right live industry forever anyone claims right job particular spouting nonsense long term federal welfare program outlived usefulness pat hum enjoy putting words mouth come nome meet miners sure things go south lower used visit course believe media news going heck plain crazy well seems alot unionist types seem think job right priviledge right job forbearers see kennedy tel see families married reason many historians poli sci types use unionist socialist breath miners know average hardworking people pay taxes earn living taxes answer maybe could move discussion appropriate newsgroup michael adams nsmca acad alaska edu high jacked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Words removal"
      ],
      "metadata": {
        "id": "WPEfPgE1E3jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WAvPh21cJzK",
        "outputId": "79f24897-7752-4812-bbd2-bfdde6b00007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fowVuAgUJG9V",
        "outputId": "53bf3bca-06f5-4bf9-e4a4-68e32cf23ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "rec.sport.hockey            600\n",
              "soc.religion.christian      599\n",
              "rec.motorcycles             598\n",
              "rec.sport.baseball          597\n",
              "sci.crypt                   595\n",
              "rec.autos                   594\n",
              "sci.med                     594\n",
              "sci.space                   593\n",
              "comp.windows.x              593\n",
              "comp.os.ms-windows.misc     591\n",
              "sci.electronics             591\n",
              "comp.sys.ibm.pc.hardware    590\n",
              "misc.forsale                585\n",
              "comp.graphics               584\n",
              "comp.sys.mac.hardware       578\n",
              "talk.politics.mideast       564\n",
              "talk.politics.guns          546\n",
              "alt.atheism                 480\n",
              "talk.politics.misc          465\n",
              "talk.religion.misc          377\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "djsr2qCZFDti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Preprocessing function with stemming\n",
        "def preprocess_and_stem(text):\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Apply stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Load the dataset\n",
        "# df = pd.read_csv('20_newsgroups_combined.csv')\n",
        "\n",
        "# Apply preprocessing and stemming to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_and_stem)\n",
        "\n",
        "# Display the first few rows of the dataframe to see the changes\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVgGSbNPEqPw",
        "outputId": "4e3463c9-b663-4005-9a81-4c729596f88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text     target\n",
            "0  nsmca aurora alaska edu subject eco freak forc...  sci.space\n",
            "1  henri zoo toronto edu henri spencer subject bi...  sci.space\n",
            "2  baalk kelvin jpl nasa gov ron baalk subject ma...  sci.space\n",
            "3  subject quotat lowest bidder bioccnt otago ac ...  sci.space\n",
            "4  jmcocker eo ncsu edu mitch subject wrench work...  sci.space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "3j3GELEeK7It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and 'target' is your target column\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target column\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Display the mapping of classes to their encoded labels\n",
        "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Class Mapping:\")\n",
        "for class_name, encoded_label in class_mapping.items():\n",
        "    print(f\"{class_name}: {encoded_label}\")\n",
        "\n",
        "# Display the DataFrame with the new encoded target column\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g645uFfwKOyV",
        "outputId": "7f3ea1bb-1e25-491f-8afa-a558c52fecaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Mapping:\n",
            "alt.atheism: 0\n",
            "comp.graphics: 1\n",
            "comp.os.ms-windows.misc: 2\n",
            "comp.sys.ibm.pc.hardware: 3\n",
            "comp.sys.mac.hardware: 4\n",
            "comp.windows.x: 5\n",
            "misc.forsale: 6\n",
            "rec.autos: 7\n",
            "rec.motorcycles: 8\n",
            "rec.sport.baseball: 9\n",
            "rec.sport.hockey: 10\n",
            "sci.crypt: 11\n",
            "sci.electronics: 12\n",
            "sci.med: 13\n",
            "sci.space: 14\n",
            "soc.religion.christian: 15\n",
            "talk.politics.guns: 16\n",
            "talk.politics.mideast: 17\n",
            "talk.politics.misc: 18\n",
            "talk.religion.misc: 19\n",
            "                                                text  target\n",
            "0  nsmca aurora alaska edu subject eco freak forc...      14\n",
            "1  henri zoo toronto edu henri spencer subject bi...      14\n",
            "2  baalk kelvin jpl nasa gov ron baalk subject ma...      14\n",
            "3  subject quotat lowest bidder bioccnt otago ac ...      14\n",
            "4  jmcocker eo ncsu edu mitch subject wrench work...      14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Creation"
      ],
      "metadata": {
        "id": "gCRyUlIuLVWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example of TF-IDF Vectorization\n",
        "# vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "# X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# # Optional: Print the vocabulary size\n",
        "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "# # Example: Print the shape of the vectorized data\n",
        "# print(f\"Shape of X: {X.shape}\")\n"
      ],
      "metadata": {
        "id": "SK7fDjr0dN3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow tensorflow-hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D8Mtn641Q-3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Embedding"
      ],
      "metadata": {
        "id": "sfSJUfQ_BmY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "\n",
        "# Load Universal Sentence Encoder\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "metadata": {
        "id": "uRYlCuN_RCpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume df['preprocessed_text'] contains your preprocessed text data\n",
        "\n",
        "# Function to convert text to embeddings\n",
        "def embed_text(text):\n",
        "    return embed([text])[0].numpy()\n",
        "\n",
        "# Example usage: Transform your text data to embeddings\n",
        "embeddings = df['text'].apply(embed_text)\n",
        "\n",
        "# Check the shape of the embeddings\n",
        "print(f\"Shape of embeddings: {embeddings.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqT1jKkTS_XG",
        "outputId": "71c45246-ecda-4ae7-cdfb-4ec2389768ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings: (11314,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=embeddings\n",
        "y=df['target']"
      ],
      "metadata": {
        "id": "BH75JBLpLwia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test-Split"
      ],
      "metadata": {
        "id": "25DxSHGSBd2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "g8RC8C-Zfxbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN(Artificial Neural Network)"
      ],
      "metadata": {
        "id": "Psd5vtbKBDNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "# Example data conversion (modify this according to your actual data structure)\n",
        "# Assuming X_train and X_test are originally lists of sequences or pandas DataFrames\n",
        "\n",
        "# Ensure the data is in the correct format (2D NumPy arrays)\n",
        "X_train = np.array([np.array(x) for x in X_train])\n",
        "X_test = np.array([np.array(x) for x in X_test])\n",
        "\n",
        "# Convert data types to float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Convert NumPy arrays to TensorFlow Tensors\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "# Building a simpler ANN model with reduced complexity\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.1)))\n",
        "model.add(Dropout(0.5))  # Dropout for regularization\n",
        "model.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.1)))\n",
        "model.add(Dropout(0.5))  # Dropout for regularization\n",
        "model.add(Dense(len(set(y_train)), activation='softmax', kernel_initializer=RandomNormal(mean=0.0, stddev=0.1)))\n",
        "\n",
        "# Compiling the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Training the simplified model with early stopping\n",
        "history = model.fit(X_train_tf, y_train, epochs=50, batch_size=64,\n",
        "                    validation_data=(X_test_tf, y_test), verbose=1, callbacks=[early_stop])\n",
        "\n",
        "# Evaluating the model on training data\n",
        "train_loss, train_accuracy = model.evaluate(X_train_tf, y_train, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluating the model on testing data\n",
        "test_loss, test_accuracy = model.evaluate(X_test_tf, y_test, verbose=0)\n",
        "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNy6CLnCMvPS",
        "outputId": "b036b531-bf07-4499-cc8d-72465ce6ab99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "142/142 [==============================] - 3s 9ms/step - loss: 2.6035 - accuracy: 0.1985 - val_loss: 1.7221 - val_accuracy: 0.5475\n",
            "Epoch 2/50\n",
            "142/142 [==============================] - 2s 12ms/step - loss: 1.6586 - accuracy: 0.4706 - val_loss: 1.1842 - val_accuracy: 0.6544\n",
            "Epoch 3/50\n",
            "142/142 [==============================] - 1s 8ms/step - loss: 1.3528 - accuracy: 0.5618 - val_loss: 1.0378 - val_accuracy: 0.6796\n",
            "Epoch 4/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 1.2052 - accuracy: 0.6121 - val_loss: 0.9552 - val_accuracy: 0.7030\n",
            "Epoch 5/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 1.1145 - accuracy: 0.6384 - val_loss: 0.9145 - val_accuracy: 0.7061\n",
            "Epoch 6/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 1.0428 - accuracy: 0.6655 - val_loss: 0.8848 - val_accuracy: 0.7123\n",
            "Epoch 7/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.9782 - accuracy: 0.6807 - val_loss: 0.8606 - val_accuracy: 0.7234\n",
            "Epoch 8/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.9423 - accuracy: 0.6999 - val_loss: 0.8392 - val_accuracy: 0.7331\n",
            "Epoch 9/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.8961 - accuracy: 0.7135 - val_loss: 0.8243 - val_accuracy: 0.7304\n",
            "Epoch 10/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.8703 - accuracy: 0.7243 - val_loss: 0.8140 - val_accuracy: 0.7331\n",
            "Epoch 11/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.8404 - accuracy: 0.7326 - val_loss: 0.7979 - val_accuracy: 0.7402\n",
            "Epoch 12/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.7997 - accuracy: 0.7410 - val_loss: 0.7984 - val_accuracy: 0.7388\n",
            "Epoch 13/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.7457 - val_loss: 0.7818 - val_accuracy: 0.7468\n",
            "Epoch 14/50\n",
            "142/142 [==============================] - 1s 9ms/step - loss: 0.7515 - accuracy: 0.7609 - val_loss: 0.7896 - val_accuracy: 0.7411\n",
            "Epoch 15/50\n",
            "142/142 [==============================] - 1s 10ms/step - loss: 0.7425 - accuracy: 0.7609 - val_loss: 0.7883 - val_accuracy: 0.7362\n",
            "Epoch 16/50\n",
            "142/142 [==============================] - 1s 10ms/step - loss: 0.7089 - accuracy: 0.7720 - val_loss: 0.7723 - val_accuracy: 0.7455\n",
            "Epoch 17/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.6897 - accuracy: 0.7800 - val_loss: 0.7707 - val_accuracy: 0.7450\n",
            "Epoch 18/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.6709 - accuracy: 0.7844 - val_loss: 0.7666 - val_accuracy: 0.7508\n",
            "Epoch 19/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.6541 - accuracy: 0.7864 - val_loss: 0.7650 - val_accuracy: 0.7468\n",
            "Epoch 20/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.6221 - accuracy: 0.7958 - val_loss: 0.7643 - val_accuracy: 0.7517\n",
            "Epoch 21/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.6149 - accuracy: 0.8018 - val_loss: 0.7550 - val_accuracy: 0.7499\n",
            "Epoch 22/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.5925 - accuracy: 0.8075 - val_loss: 0.7587 - val_accuracy: 0.7552\n",
            "Epoch 23/50\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.5734 - accuracy: 0.8104 - val_loss: 0.7622 - val_accuracy: 0.7548\n",
            "Epoch 24/50\n",
            "142/142 [==============================] - 1s 7ms/step - loss: 0.5757 - accuracy: 0.8113 - val_loss: 0.7581 - val_accuracy: 0.7587\n",
            "Training Accuracy: 0.8885\n",
            "Testing Accuracy: 0.7499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Learning"
      ],
      "metadata": {
        "id": "EOtRrDfYBBKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Ensure the data is in the correct format (2D NumPy arrays)\n",
        "X_train = np.array([np.array(x) for x in X_train])\n",
        "X_test = np.array([np.array(x) for x in X_test])\n",
        "\n",
        "# Convert data types to float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Splitting training data for validation\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different architectures with reduced complexity\n",
        "def create_model_1():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_model_2():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_model_3():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax',\n",
        "                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.1),\n",
        "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the models with reduced complexity\n",
        "models = []\n",
        "for create_model in [create_model_1, create_model_2, create_model_3]:\n",
        "    model = create_model()\n",
        "    model.fit(X_train_split, y_train_split, epochs=50, batch_size=64,\n",
        "              validation_data=(X_val_split, y_val_split), verbose=1,\n",
        "              callbacks=[early_stopping])\n",
        "    models.append(model)\n",
        "\n",
        "# Collect predictions\n",
        "train_meta_features = np.zeros((X_train.shape[0], len(models) * len(set(y_train))))\n",
        "test_meta_features = np.zeros((X_test.shape[0], len(models) * len(set(y_train))))\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    train_meta_features[:, i * len(set(y_train)):(i + 1) * len(set(y_train))] = model.predict(X_train, verbose=0)\n",
        "    test_meta_features[:, i * len(set(y_train)):(i + 1) * len(set(y_train))] = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Standardize meta-features\n",
        "scaler = StandardScaler()\n",
        "train_meta_features = scaler.fit_transform(train_meta_features)\n",
        "test_meta_features = scaler.transform(test_meta_features)\n",
        "\n",
        "# Use a meta-learner for stacking\n",
        "meta_learner = LogisticRegression(max_iter=1000)\n",
        "meta_learner.fit(train_meta_features, y_train)\n",
        "\n",
        "# Make final predictions\n",
        "final_predictions = meta_learner.predict(test_meta_features)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "ensemble_accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f\"Stacking Ensemble Testing Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate each model on training data\n",
        "for i, model in enumerate(models):\n",
        "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "    print(f\"Model {i+1} Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate each model on testing data\n",
        "for i, model in enumerate(models):\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Model {i+1} Testing Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7JTonQr9lR3",
        "outputId": "73677cf8-bb8e-48d9-e329-54dcaac9004a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "114/114 [==============================] - 2s 9ms/step - loss: 3.1606 - accuracy: 0.1124 - val_loss: 2.9045 - val_accuracy: 0.2269\n",
            "Epoch 2/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 2.5614 - accuracy: 0.2548 - val_loss: 2.2181 - val_accuracy: 0.3821\n",
            "Epoch 3/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 2.1761 - accuracy: 0.3599 - val_loss: 1.9943 - val_accuracy: 0.4677\n",
            "Epoch 4/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 2.0253 - accuracy: 0.4124 - val_loss: 1.8761 - val_accuracy: 0.4887\n",
            "Epoch 5/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.9356 - accuracy: 0.4543 - val_loss: 1.8038 - val_accuracy: 0.5130\n",
            "Epoch 6/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.8618 - accuracy: 0.4827 - val_loss: 1.7527 - val_accuracy: 0.5417\n",
            "Epoch 7/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.8272 - accuracy: 0.4914 - val_loss: 1.7014 - val_accuracy: 0.5577\n",
            "Epoch 8/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.7888 - accuracy: 0.5126 - val_loss: 1.6772 - val_accuracy: 0.5820\n",
            "Epoch 9/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.7467 - accuracy: 0.5285 - val_loss: 1.6388 - val_accuracy: 0.5875\n",
            "Epoch 10/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.7301 - accuracy: 0.5436 - val_loss: 1.6158 - val_accuracy: 0.5914\n",
            "Epoch 11/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.7055 - accuracy: 0.5548 - val_loss: 1.6027 - val_accuracy: 0.5991\n",
            "Epoch 12/50\n",
            "114/114 [==============================] - 1s 9ms/step - loss: 1.6845 - accuracy: 0.5597 - val_loss: 1.5758 - val_accuracy: 0.6168\n",
            "Epoch 13/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.6662 - accuracy: 0.5720 - val_loss: 1.5697 - val_accuracy: 0.6157\n",
            "Epoch 14/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.6543 - accuracy: 0.5735 - val_loss: 1.5508 - val_accuracy: 0.6234\n",
            "Epoch 15/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.6411 - accuracy: 0.5804 - val_loss: 1.5360 - val_accuracy: 0.6306\n",
            "Epoch 16/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.5866 - val_loss: 1.5298 - val_accuracy: 0.6322\n",
            "Epoch 17/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.6199 - accuracy: 0.5934 - val_loss: 1.5202 - val_accuracy: 0.6383\n",
            "Epoch 18/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.6079 - accuracy: 0.6055 - val_loss: 1.5222 - val_accuracy: 0.6367\n",
            "Epoch 19/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5898 - accuracy: 0.6014 - val_loss: 1.5108 - val_accuracy: 0.6367\n",
            "Epoch 20/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.5837 - accuracy: 0.6075 - val_loss: 1.4992 - val_accuracy: 0.6433\n",
            "Epoch 21/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5729 - accuracy: 0.6117 - val_loss: 1.4906 - val_accuracy: 0.6505\n",
            "Epoch 22/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5632 - accuracy: 0.6088 - val_loss: 1.4989 - val_accuracy: 0.6461\n",
            "Epoch 23/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5729 - accuracy: 0.6123 - val_loss: 1.4869 - val_accuracy: 0.6549\n",
            "Epoch 24/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.5565 - accuracy: 0.6247 - val_loss: 1.4789 - val_accuracy: 0.6549\n",
            "Epoch 25/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5537 - accuracy: 0.6269 - val_loss: 1.4797 - val_accuracy: 0.6543\n",
            "Epoch 26/50\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 1.5467 - accuracy: 0.6290 - val_loss: 1.4743 - val_accuracy: 0.6543\n",
            "Epoch 27/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.5335 - accuracy: 0.6298 - val_loss: 1.4658 - val_accuracy: 0.6610\n",
            "Epoch 28/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5362 - accuracy: 0.6282 - val_loss: 1.4749 - val_accuracy: 0.6582\n",
            "Epoch 29/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.5323 - accuracy: 0.6370 - val_loss: 1.4617 - val_accuracy: 0.6681\n",
            "Epoch 30/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.5268 - accuracy: 0.6369 - val_loss: 1.4632 - val_accuracy: 0.6698\n",
            "Epoch 31/50\n",
            "114/114 [==============================] - 1s 10ms/step - loss: 1.5386 - accuracy: 0.6319 - val_loss: 1.4738 - val_accuracy: 0.6665\n",
            "Epoch 32/50\n",
            "114/114 [==============================] - 1s 9ms/step - loss: 1.5293 - accuracy: 0.6370 - val_loss: 1.4690 - val_accuracy: 0.6753\n",
            "Epoch 33/50\n",
            "114/114 [==============================] - 1s 9ms/step - loss: 1.5284 - accuracy: 0.6419 - val_loss: 1.4670 - val_accuracy: 0.6726\n",
            "Epoch 34/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5155 - accuracy: 0.6435 - val_loss: 1.4589 - val_accuracy: 0.6726\n",
            "Epoch 35/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5029 - accuracy: 0.6490 - val_loss: 1.4619 - val_accuracy: 0.6764\n",
            "Epoch 36/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5022 - accuracy: 0.6559 - val_loss: 1.4583 - val_accuracy: 0.6698\n",
            "Epoch 37/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.5049 - accuracy: 0.6457 - val_loss: 1.4559 - val_accuracy: 0.6687\n",
            "Epoch 38/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5050 - accuracy: 0.6483 - val_loss: 1.4541 - val_accuracy: 0.6715\n",
            "Epoch 39/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5054 - accuracy: 0.6555 - val_loss: 1.4532 - val_accuracy: 0.6770\n",
            "Epoch 40/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5106 - accuracy: 0.6485 - val_loss: 1.4520 - val_accuracy: 0.6842\n",
            "Epoch 41/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5076 - accuracy: 0.6490 - val_loss: 1.4460 - val_accuracy: 0.6759\n",
            "Epoch 42/50\n",
            "114/114 [==============================] - 1s 10ms/step - loss: 1.4917 - accuracy: 0.6570 - val_loss: 1.4476 - val_accuracy: 0.6825\n",
            "Epoch 43/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4959 - accuracy: 0.6568 - val_loss: 1.4500 - val_accuracy: 0.6781\n",
            "Epoch 44/50\n",
            "114/114 [==============================] - 1s 10ms/step - loss: 1.4844 - accuracy: 0.6594 - val_loss: 1.4508 - val_accuracy: 0.6825\n",
            "Epoch 45/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4871 - accuracy: 0.6605 - val_loss: 1.4374 - val_accuracy: 0.6880\n",
            "Epoch 46/50\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.4808 - accuracy: 0.6651 - val_loss: 1.4439 - val_accuracy: 0.6825\n",
            "Epoch 47/50\n",
            "114/114 [==============================] - 1s 11ms/step - loss: 1.4880 - accuracy: 0.6569 - val_loss: 1.4453 - val_accuracy: 0.6864\n",
            "Epoch 48/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.4862 - accuracy: 0.6646 - val_loss: 1.4448 - val_accuracy: 0.6853\n",
            "Epoch 49/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.4723 - accuracy: 0.6657 - val_loss: 1.4376 - val_accuracy: 0.6886\n",
            "Epoch 50/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.4744 - accuracy: 0.6713 - val_loss: 1.4364 - val_accuracy: 0.6880\n",
            "Epoch 1/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 3.3029 - accuracy: 0.1692 - val_loss: 2.6938 - val_accuracy: 0.3611\n",
            "Epoch 2/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 2.3639 - accuracy: 0.3717 - val_loss: 2.0294 - val_accuracy: 0.4975\n",
            "Epoch 3/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.9829 - accuracy: 0.4809 - val_loss: 1.8019 - val_accuracy: 0.5516\n",
            "Epoch 4/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.8256 - accuracy: 0.5366 - val_loss: 1.6945 - val_accuracy: 0.6019\n",
            "Epoch 5/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.7392 - accuracy: 0.5669 - val_loss: 1.6179 - val_accuracy: 0.6383\n",
            "Epoch 6/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.6714 - accuracy: 0.5902 - val_loss: 1.5827 - val_accuracy: 0.6438\n",
            "Epoch 7/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.6366 - accuracy: 0.6044 - val_loss: 1.5408 - val_accuracy: 0.6571\n",
            "Epoch 8/50\n",
            "114/114 [==============================] - 1s 12ms/step - loss: 1.5972 - accuracy: 0.6229 - val_loss: 1.5236 - val_accuracy: 0.6610\n",
            "Epoch 9/50\n",
            "114/114 [==============================] - 1s 11ms/step - loss: 1.5644 - accuracy: 0.6385 - val_loss: 1.5040 - val_accuracy: 0.6626\n",
            "Epoch 10/50\n",
            "114/114 [==============================] - 1s 12ms/step - loss: 1.5489 - accuracy: 0.6355 - val_loss: 1.4809 - val_accuracy: 0.6864\n",
            "Epoch 11/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.5314 - accuracy: 0.6551 - val_loss: 1.4701 - val_accuracy: 0.6858\n",
            "Epoch 12/50\n",
            "114/114 [==============================] - 1s 4ms/step - loss: 1.5198 - accuracy: 0.6541 - val_loss: 1.4518 - val_accuracy: 0.6941\n",
            "Epoch 13/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4952 - accuracy: 0.6669 - val_loss: 1.4417 - val_accuracy: 0.6930\n",
            "Epoch 14/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4835 - accuracy: 0.6720 - val_loss: 1.4322 - val_accuracy: 0.7035\n",
            "Epoch 15/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4804 - accuracy: 0.6744 - val_loss: 1.4311 - val_accuracy: 0.6980\n",
            "Epoch 16/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4664 - accuracy: 0.6833 - val_loss: 1.4172 - val_accuracy: 0.7073\n",
            "Epoch 17/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4552 - accuracy: 0.6888 - val_loss: 1.4144 - val_accuracy: 0.7079\n",
            "Epoch 18/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4506 - accuracy: 0.6884 - val_loss: 1.4267 - val_accuracy: 0.7051\n",
            "Epoch 19/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4390 - accuracy: 0.6959 - val_loss: 1.4240 - val_accuracy: 0.7002\n",
            "Epoch 20/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4427 - accuracy: 0.6859 - val_loss: 1.4100 - val_accuracy: 0.7101\n",
            "Epoch 21/50\n",
            "114/114 [==============================] - 1s 4ms/step - loss: 1.4346 - accuracy: 0.6979 - val_loss: 1.4103 - val_accuracy: 0.7079\n",
            "Epoch 22/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4169 - accuracy: 0.7011 - val_loss: 1.4175 - val_accuracy: 0.7096\n",
            "Epoch 23/50\n",
            "114/114 [==============================] - 1s 4ms/step - loss: 1.4208 - accuracy: 0.6988 - val_loss: 1.4005 - val_accuracy: 0.7140\n",
            "Epoch 24/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4091 - accuracy: 0.7070 - val_loss: 1.4029 - val_accuracy: 0.7140\n",
            "Epoch 25/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4090 - accuracy: 0.7036 - val_loss: 1.4076 - val_accuracy: 0.7073\n",
            "Epoch 26/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.4002 - accuracy: 0.7122 - val_loss: 1.3938 - val_accuracy: 0.7178\n",
            "Epoch 27/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3943 - accuracy: 0.7097 - val_loss: 1.3907 - val_accuracy: 0.7129\n",
            "Epoch 28/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3935 - accuracy: 0.7156 - val_loss: 1.4021 - val_accuracy: 0.7151\n",
            "Epoch 29/50\n",
            "114/114 [==============================] - 1s 4ms/step - loss: 1.3939 - accuracy: 0.7120 - val_loss: 1.3906 - val_accuracy: 0.7156\n",
            "Epoch 30/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.3866 - accuracy: 0.7191 - val_loss: 1.3865 - val_accuracy: 0.7156\n",
            "Epoch 31/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.3774 - accuracy: 0.7186 - val_loss: 1.4010 - val_accuracy: 0.7156\n",
            "Epoch 32/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.3862 - accuracy: 0.7177 - val_loss: 1.3922 - val_accuracy: 0.7234\n",
            "Epoch 33/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.3766 - accuracy: 0.7253 - val_loss: 1.3904 - val_accuracy: 0.7211\n",
            "Epoch 34/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3756 - accuracy: 0.7217 - val_loss: 1.3841 - val_accuracy: 0.7245\n",
            "Epoch 35/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3741 - accuracy: 0.7238 - val_loss: 1.3868 - val_accuracy: 0.7206\n",
            "Epoch 36/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3671 - accuracy: 0.7247 - val_loss: 1.3788 - val_accuracy: 0.7267\n",
            "Epoch 37/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3744 - accuracy: 0.7260 - val_loss: 1.3921 - val_accuracy: 0.7239\n",
            "Epoch 38/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3600 - accuracy: 0.7267 - val_loss: 1.3921 - val_accuracy: 0.7245\n",
            "Epoch 39/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3692 - accuracy: 0.7239 - val_loss: 1.3823 - val_accuracy: 0.7294\n",
            "Epoch 40/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3619 - accuracy: 0.7333 - val_loss: 1.3852 - val_accuracy: 0.7239\n",
            "Epoch 41/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3625 - accuracy: 0.7285 - val_loss: 1.3784 - val_accuracy: 0.7272\n",
            "Epoch 42/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3568 - accuracy: 0.7387 - val_loss: 1.3792 - val_accuracy: 0.7245\n",
            "Epoch 43/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3558 - accuracy: 0.7316 - val_loss: 1.3778 - val_accuracy: 0.7278\n",
            "Epoch 44/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3604 - accuracy: 0.7349 - val_loss: 1.3858 - val_accuracy: 0.7223\n",
            "Epoch 45/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3547 - accuracy: 0.7294 - val_loss: 1.3817 - val_accuracy: 0.7239\n",
            "Epoch 46/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3518 - accuracy: 0.7354 - val_loss: 1.3850 - val_accuracy: 0.7239\n",
            "Epoch 47/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3565 - accuracy: 0.7304 - val_loss: 1.3776 - val_accuracy: 0.7261\n",
            "Epoch 48/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3472 - accuracy: 0.7381 - val_loss: 1.3772 - val_accuracy: 0.7234\n",
            "Epoch 49/50\n",
            "114/114 [==============================] - 0s 4ms/step - loss: 1.3413 - accuracy: 0.7421 - val_loss: 1.3767 - val_accuracy: 0.7322\n",
            "Epoch 50/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.3359 - accuracy: 0.7445 - val_loss: 1.3774 - val_accuracy: 0.7294\n",
            "Epoch 1/50\n",
            "114/114 [==============================] - 2s 7ms/step - loss: 3.7476 - accuracy: 0.2713 - val_loss: 2.7116 - val_accuracy: 0.5097\n",
            "Epoch 2/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 2.3678 - accuracy: 0.5341 - val_loss: 2.0086 - val_accuracy: 0.6411\n",
            "Epoch 3/50\n",
            "114/114 [==============================] - 1s 10ms/step - loss: 1.9569 - accuracy: 0.6189 - val_loss: 1.7835 - val_accuracy: 0.6803\n",
            "Epoch 4/50\n",
            "114/114 [==============================] - 1s 10ms/step - loss: 1.7774 - accuracy: 0.6490 - val_loss: 1.6509 - val_accuracy: 0.6924\n",
            "Epoch 5/50\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.6573 - accuracy: 0.6767 - val_loss: 1.5865 - val_accuracy: 0.6996\n",
            "Epoch 6/50\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5914 - accuracy: 0.6865 - val_loss: 1.5421 - val_accuracy: 0.7162\n",
            "Epoch 7/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.5400 - accuracy: 0.6967 - val_loss: 1.5103 - val_accuracy: 0.7123\n",
            "Epoch 8/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5089 - accuracy: 0.7004 - val_loss: 1.4852 - val_accuracy: 0.7118\n",
            "Epoch 9/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4741 - accuracy: 0.7076 - val_loss: 1.4514 - val_accuracy: 0.7256\n",
            "Epoch 10/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4468 - accuracy: 0.7185 - val_loss: 1.4418 - val_accuracy: 0.7173\n",
            "Epoch 11/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4326 - accuracy: 0.7199 - val_loss: 1.4266 - val_accuracy: 0.7278\n",
            "Epoch 12/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4129 - accuracy: 0.7256 - val_loss: 1.4164 - val_accuracy: 0.7256\n",
            "Epoch 13/50\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 1.4031 - accuracy: 0.7249 - val_loss: 1.3997 - val_accuracy: 0.7311\n",
            "Epoch 14/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3829 - accuracy: 0.7356 - val_loss: 1.4137 - val_accuracy: 0.7289\n",
            "Epoch 15/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3772 - accuracy: 0.7282 - val_loss: 1.4166 - val_accuracy: 0.7217\n",
            "Epoch 16/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3756 - accuracy: 0.7311 - val_loss: 1.4067 - val_accuracy: 0.7366\n",
            "Epoch 17/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3638 - accuracy: 0.7399 - val_loss: 1.4148 - val_accuracy: 0.7272\n",
            "Epoch 18/50\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.3671 - accuracy: 0.7326 - val_loss: 1.3998 - val_accuracy: 0.7355\n",
            "Stacking Ensemble Testing Accuracy: 0.7260\n",
            "Model 1 Training Accuracy: 0.7376\n",
            "Model 2 Training Accuracy: 0.7881\n",
            "Model 3 Training Accuracy: 0.7706\n",
            "Model 1 Testing Accuracy: 0.6659\n",
            "Model 2 Testing Accuracy: 0.7097\n",
            "Model 3 Testing Accuracy: 0.7119\n"
          ]
        }
      ]
    }
  ]
}